{"cells":[{"metadata":{"id":"F68298420C154AE5A017CF5BDF69310C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 模型选择、过拟合和欠拟合\n- **模型选择**\n\t1.**验证数据集**\n\t从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。\n\t2.**K折交叉验证**   \n\t由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。\n\t\n\n- **过拟合和欠拟合** \n\t模型训练中经常出现的两类典型问题：\n一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；\n另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。\n在实践中，我们要尽可能同时应对欠拟合和过拟合。\n\t1.**模型复杂度**\n\t给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数\n$$\n \\hat{y} = b + \\sum_{k=1}^K x^k w_k \n$$\n来近似 $y$。在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。\n给定训练数据集，模型复杂度和误差之间的关系：\n![Image Name](https://cdn.kesci.com/upload/image/q5jc27wxoj.png?imageView2/0/w/960/h/960)\n\t2.**训练数据集大小**\n\t影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。\n\t\n- **解决方法**\n\t1.**正则化**\n\t通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。\n\t$L_2$范数正则化令**权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度**。因此，$L_2$范数正则化又叫**权重衰减**。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。\n\t2.**丢弃法**\n\t多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \\ldots, 5$）的计算表达式为\n$$\n h_i = \\phi\\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\\right) \n$$\n这里$\\phi$是激活函数，$x_1, \\ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \\ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i'$\n$$\n h_i' = \\frac{\\xi_i}{1-p} h_i \n$$\n由于$E(\\xi_i) = 1-p$，因此\n$$\n E(h_i') = \\frac{E(\\xi_i)}{1-p}h_i = h_i \n$$\n即丢弃法不改变其输入的期望值。让我们对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \\ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \\ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法\n![Image Name](https://cdn.kesci.com/upload/image/q5jd69in3m.png?imageView2/0/w/960/h/960)"},{"metadata":{"id":"CD1A38B37FB2489C9B5E7C5240A11AEB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def dropout(X, drop_prob):\n    X = X.float()\n    assert 0 <= drop_prob <= 1\n    keep_prob = 1 - drop_prob\n    # 这种情况下把全部元素都丢弃\n    if keep_prob == 0:\n        return torch.zeros_like(X)\n    mask = (torch.rand(X.shape) < keep_prob).float()\n    \n    return mask * X / keep_prob\n    \ndrop_prob1, drop_prob2 = 0.2, 0.5\ndef net(X, is_training=True):\n    X = X.view(-1, num_inputs)\n    H1 = (torch.matmul(X, W1) + b1).relu()\n    if is_training:  # 只在训练模型时使用丢弃法\n        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层\n    H2 = (torch.matmul(H1, W2) + b2).relu()\n    if is_training:\n        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层\n    return torch.matmul(H2, W3) + b3","execution_count":1},{"metadata":{"cell_type":"code","id":"509C94777E2E416D86BC6AD4DAEE8DBA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 梯度消失和梯度爆炸\n深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。\n- **当神经网络的层数较多时，模型的数值稳定性容易变差。**\n\t假设一个层数为$L$的多层感知机的第$l$层$\\boldsymbol{H}^{(l)}$的权重参数为$\\boldsymbol{W}^{(l)}$，输出层$\\boldsymbol{H}^{(L)}$的权重参数为$\\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\\phi(x) = x$。给定输入$\\boldsymbol{X}$，多层感知机的第$l$层的输出$\\boldsymbol{H}^{(l)} = \\boldsymbol{X} \\boldsymbol{W}^{(1)} \\boldsymbol{W}^{(2)} \\ldots \\boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\\boldsymbol{X}$分别与$0.2^{30} \\approx 1 \\times 10^{-21}$（消失）和$5^{30} \\approx 9 \\times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。\n\t\n- **随机初始化模型参数的原因**\n\t假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。\n\t1. PyTorch的默认随机初始化 \n\t 在线性回归的简洁实现中，我们使用`torch.nn.init.normal_()`使模型`net`的权重参数采用正态分布的随机初始化方式。不过，PyTorch中`nn.Module`的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考[源代码](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules)），因此一般不用我们考虑\n\t2. Xavier随机初始化\n\t假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布\n$$\nU\\left(-\\sqrt{\\frac{6}{a+b}}, \\sqrt{\\frac{6}{a+b}}\\right).\n$$\n它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。"},{"metadata":{"id":"A3C74B45AD414C969E95EB6A345A2EB0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 循环神经网络进阶\n\n### GRU\nRNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）  \nGRU⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系 ,解决长期记忆和反向传播中的梯度等问题。\n• 重置⻔有助于捕捉时间序列⾥短期的依赖关系；  \n• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。 \n\n- **GRU的输入输出结构**\nGRU的输入输出结构与普通的RNN是一样的。\n有一个当前的输入$x^t$ ，和上一个节点传递下来的隐状态（hidden state）$h^{t-1}$，这个隐状态包含了之前节点的相关信息。\n结合$x^t$ 和$h^{t-1}$，GRU会得到当前隐藏节点的输出$y^t$ 和传递给下一个节点的隐状态$h^t$。\n![Image Name](https://cdn.kesci.com/upload/image/q5srwnnmv7.png?imageView2/0/w/360/h/360)\n\n- **GRU的内部结构**\n首先，我们先通过上一个传输下来的状态 $h^{t-1}$ 和当前节点的输入$x^t$ 来获取两个门控状态。如下图所示，其中$r$控制重置的门控（reset gate），$z$为控制更新的门控（update gate）。\n![Image Name](https://cdn.kesci.com/upload/image/q5ss7lod3r.png?imageView2/0/w/360/h/360)\n\n得到门控信号之后，首先使用重置门控来得到“重置”之后的数据$h^{t-1'}=h^{t-1}⊙r$ ，再将$h^{t-1'}$与输入 $x^r$ 进行拼接，再通过一个tanh激活函数来将数据放缩到-1~1的范围内。即得到如下图所示的$h'$ 。\n![Image Name](https://cdn.kesci.com/upload/image/q5ssjfuxhf.png?imageView2/0/w/960/h/960)\n\n这里的$h'$主要是包含了当前输入的$x^t$数据。有针对性地对$h'$添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“\n![Image Name](https://cdn.kesci.com/upload/image/q5ssnlsje7.png?imageView2/0/w/960/h/960)\n\n最后介绍GRU最关键的一个步骤**更新记忆**阶段。\n\n在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控$z$（update gate）。\n更新表达式：$h^t = z ⊙h^{t-1}+(1-z)⊙h^{'}$\n\n首先再次强调一下，门控信号（z）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。\n\nGRU很聪明的一点就在于，我们使用了同一个门控$z$就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。\n\n**$z ⊙h^{t-1}$**：表示对原本隐藏状态的选择性“遗忘”。这里的$z$可以想象成遗忘门（forget gate），忘记$h^{t-1}$维度中一些不重要的信息。\n\n**$(1-z)⊙h'$** ： 表示对包含当前节点信息的$h'$进行选择性”记忆“。与上面类似，这里的$(1-z)$同理会忘记$h'$ 维度中的一些不重要的信息。或者，这里我们更应当看做是对h'$维度中的某些信息进行选择。\n\n**$h^t = z ⊙h^{t-1}+(1-z)⊙h^{'}$**:结合上述，这一步的操作就是忘记传递下来的 $h^{t-1}$中的某些维度信息，并加入当前节点输入的某些维度信息。\n可以看到，这里的遗忘$z$和选择$(1-z)$是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （z），我们就会使用包含当前输入的$h'$中所对应的权重进行弥补(1-z)。以保持一种”恒定“状态。\n\n"},{"metadata":{"id":"EC1D378FA5B6461F889253B7F7C9DE1C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"##初始化参数\nnum_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\nprint('will use', device)\n\ndef get_params():  \n    def _one(shape):\n        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32) #正态分布\n        return torch.nn.Parameter(ts, requires_grad=True)\n    def _three():\n        return (_one((num_inputs, num_hiddens)),\n                _one((num_hiddens, num_hiddens)),\n                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))\n     \n    W_xz, W_hz, b_z = _three()  # 更新门参数\n    W_xr, W_hr, b_r = _three()  # 重置门参数\n    W_xh, W_hh, b_h = _three()  # 候选隐藏状态参数\n    \n    # 输出层参数\n    W_hq = _one((num_hiddens, num_outputs))\n    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)\n    return nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])\n\ndef init_gru_state(batch_size, num_hiddens, device):   #隐藏状态初始化\n    return (torch.zeros((batch_size, num_hiddens), device=device), )\n    \n##GRU模型\ndef gru(inputs, state, params):\n    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    for X in inputs:\n        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)\n        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)\n        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)\n        H = Z * H + (1 - Z) * H_tilda\n        Y = torch.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return outputs, (H,)","execution_count":2},{"metadata":{"id":"7E4B327C87A14B5D8FF1D8A36DE57241","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### LSTM\n长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。\n\n遗忘门:控制上一时间步的记忆细胞 \n输入门:控制当前时间步的输入  \n输出门:控制从记忆细胞到隐藏状态  \n记忆细胞：⼀种特殊的隐藏状态的信息的流动 \n\n- **LSTM结构**\n\n![Image Name](https://cdn.kesci.com/upload/image/q5stl6xcdz.png?imageView2/0/w/960/h/960)\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5stmqibvz.png?imageView2/0/w/960/h/960)\n\n以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。\n\n但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。"},{"metadata":{"id":"40BE5EC1E44E4C7293C5E188B5AA8500","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"##初始化参数、隐层、模型定义\ndef get_params():\n    def _one(shape):\n        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n        return torch.nn.Parameter(ts, requires_grad=True)\n    def _three():\n        return (_one((num_inputs, num_hiddens)),\n                _one((num_hiddens, num_hiddens)),\n                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))\n          \n    W_xi, W_hi, b_i = _three()  # 输入门参数\n    W_xf, W_hf, b_f = _three()  # 遗忘门参数\n    W_xo, W_ho, b_o = _three()  # 输出门参数\n    W_xc, W_hc, b_c = _three()  # 候选记忆细胞参数\n          \n    # 输出层参数\n    W_hq = _one((num_hiddens, num_outputs))\n    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)\n    return nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])\n          \n          \ndef init_lstm_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), \n            torch.zeros((batch_size, num_hiddens), device=device))\n                  \n      \ndef lstm(inputs, state, params):\n    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params\n    (H, C) = state\n    outputs = []\n    for X in inputs:\n        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)\n        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)\n        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)\n        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)\n        C = F * C + I * C_tilda\n        H = O * C.tanh()\n        Y = torch.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return outputs, (H, C)","execution_count":3},{"metadata":{"id":"9CA727FD0E904AA39A1C52F018BD37BF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 深度循环神经网络  \n\n![Image Name](https://cdn.kesci.com/upload/image/q5jk3z1hvz.png?imageView2/0/w/320/h/320)\n\n$$\n\\boldsymbol{H}_t^{(1)} = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(1)} + \\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{hh}^{(1)} + \\boldsymbol{b}_h^{(1)})\\\\\n\\boldsymbol{H}_t^{(\\ell)} = \\phi(\\boldsymbol{H}_t^{(\\ell-1)} \\boldsymbol{W}_{xh}^{(\\ell)} + \\boldsymbol{H}_{t-1}^{(\\ell)} \\boldsymbol{W}_{hh}^{(\\ell)} + \\boldsymbol{b}_h^{(\\ell)})\\\\\n\\boldsymbol{O}_t = \\boldsymbol{H}_t^{(L)} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n$$"},{"metadata":{"id":"D49649152A84435CB8057457658FA456","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 双向循环神经网络 \n\n![Image Name](https://cdn.kesci.com/upload/image/q5j8hmgyrz.png?imageView2/0/w/320/h/320)\n\n$$ \n\\begin{aligned} \\overrightarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(f)} + \\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{hh}^{(f)} + \\boldsymbol{b}_h^{(f)})\\\\\n\\overleftarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(b)} + \\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{hh}^{(b)} + \\boldsymbol{b}_h^{(b)}) \\end{aligned} $$\n$$\n\\boldsymbol{H}_t=(\\overrightarrow{\\boldsymbol{H}}_{t}, \\overleftarrow{\\boldsymbol{H}}_t)\n$$\n$$\n\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n$$"},{"metadata":{"id":"8CC05C1EB07A41CC9C0A884549B51AC0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 机器翻译\n机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。\n主要特征：输出是单词序列而不是单个单词。 \n困难：输出序列的长度可能与源序列的长度不同。\n### 数据处理\n1.数据清洗：处理乱码与空格。\n2.分词：将字符串转换成单词组成的列表\n3.建立词典，将单词组成的列表编程单词id组成的列表，这里会得到如下几样东西\n\t\t> 去重后词典，及其中单词对应的索引列表\n\t\t> 还可以得到给定索引找到其对应的单词的列表，以及给定单词得到对应索引的字典。\n\t\t> 原始语料所有词对应的词典索引的列表\n4.padding:一个batch中所有句子输入长度保持一致\n\t```\n\tdef pad(line, max_len, padding_token):\n\t\t\tif len(line) > max_len:\n\t\t\t\t\treturn line[:max_len]\n\t\t\treturn line + [padding_token] * (max_len - len(line))\n\t```\n5.加载数据：数据生成器\n\n### Encoder-Decoder\nencoder：输入到隐藏状态  \ndecoder：隐藏状态到输出\n对话系统、生成式任务、翻译\n\n####  Sequence to Sequence模型\nseq2seq模型基于编码器-解码器体系结构，通过序列输入生成序列输出。 \n编码器和解码器都使用递归神经网络（RNN）处理可变长度的序列输入。 \n编码器的隐藏状态直接用于初始化解码器的隐藏状态，以将信息从编码器传递到解码器。\n\n训练  \n![Image Name](https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640)\n预测\n![Image Name](https://cdn.kesci.com/upload/image/q5jcecxcba.png?imageView2/0/w/640/h/640)\n具体结构：\n![Image Name](https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500)\n\nEncoder\n```\nclass Seq2SeqEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqEncoder, self).__init__(**kwargs)\n        self.num_hiddens=num_hiddens\n        self.num_layers=num_layers\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n   \n    def begin_state(self, batch_size, device):\n        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),\n                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]\n    def forward(self, X, *args):\n        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)\n        X = X.transpose(0, 1)  # RNN needs first axes to be time\n        # state = self.begin_state(X.shape[1], device=X.device)\n        out, state = self.rnn(X)\n        # The shape of out is (seq_len, batch_size, num_hiddens).\n        # state contains the hidden state and the memory cell\n        # of the last time step, the shape is (num_layers, batch_size, num_hiddens)\n        return out, state\n```\nDecoder\n```\nclass Seq2SeqDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqDecoder, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Linear(num_hiddens,vocab_size)\n\n    def init_state(self, enc_outputs, *args):\n        return enc_outputs[1]\n\n    def forward(self, X, state):\n        X = self.embedding(X).transpose(0, 1)\n        out, state = self.rnn(X, state)\n        # Make the batch to be the first dimension to simplify loss computation.\n        out = self.dense(out).transpose(0, 1)\n        return out, state\n```\n\n### 损失函数\n解码器的输出是一个和词典维度相同的向量，其每个值对应与向量索引位置对应词的分数，一般是选择分数最大的那个词作为最终的输出。\n在计算损失函数之前，要把padding去掉，因为padding的部分不参与计算\n#### 序列屏蔽\n序列有效长度保留，无效长度填充为特定value\n```\ndef SequenceMask(X, X_len,value=0):\n    maxlen = X.size(1)\n    mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]   \n    X[~mask]=value\n    return X\n```\n#### 损失函数MaskedSoftmaxCELoss\n在交叉熵的基础上加上SequenceMask\n```\nclass MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n    # pred shape: (batch_size, seq_len, vocab_size)\n    # label shape: (batch_size, seq_len)\n    # valid_length shape: (batch_size, )\n    def forward(self, pred, label, valid_length):\n        # the sample weights shape should be (batch_size, seq_len)\n        weights = torch.ones_like(label)\n        weights = SequenceMask(weights, valid_length).float()\n        self.reduction='none'\n        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)\n        return (output*weights).mean(dim=1)\n```\n\n###  Beam Search搜索算法\ngreedy search:只考虑当前时刻的局部最优解，没有考虑前后语义是否连贯（非全局最优解)\n维特比算法:选择整体分数最高的句子（搜索空间太大）\n集束搜索：结合了greedy search和维特比算法\n![Image Name](https://cdn.kesci.com/upload/image/q5jcia86z1.png?imageView2/0/w/640/h/640)\n"},{"metadata":{"id":"46957AA95325436E97DD9446BB181F5D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 注意力机制\n### seq2seq的不足\n解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，**所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降**。\n与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把“Hello world”翻译成“Bonjour le monde”时，“Hello”映射成“Bonjour”，“world”映射成“monde”。在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。\n\n### 注意力机制框架\n\nAttention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。$𝐤_𝑖∈ℝ^{𝑑_𝑘}, 𝐯_𝑖∈ℝ^{𝑑_𝑣}$. Query  $𝐪∈ℝ^{𝑑_𝑞}$ , attention layer得到输出与value的维度一致 $𝐨∈ℝ^{𝑑_𝑣}$. 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量$o$则是value的加权求和，而每个key计算的权重与value一一对应。\n\n为了计算输出，我们首先假设有一个函数$\\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \\ldots, a_n$ by\n$$\na_i = \\alpha(\\mathbf q, \\mathbf k_i).\n$$\n我们使用 softmax函数 获得注意力权重：\n$$\nb_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n).\n$$\n最终的输出就是value的加权求和：\n$$\n\\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i.\n$$\n![Image Name](https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960)\n\n不同的attetion layer的区别在于score函数的选择，在本节的其余部分，我们将讨论两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention\n\n\n### 点积注意力\nThe dot product 假设query和keys有相同的维度, 即 $\\forall i, 𝐪,𝐤_𝑖 ∈ ℝ_𝑑 $. 通过计算query和key转置的乘积来计算attention score,通常还会除去 $\\sqrt{d}$ 减少计算出来的score对维度𝑑的依赖性，如下\n$$\n𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \\sqrt{d} \n$$\n假设 $ 𝐐∈ℝ^{𝑚×𝑑}$ 有 $m$ 个query，$𝐊∈ℝ^{𝑛×𝑑}$ 有 $n$ 个keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个score：\n$$\n𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\\sqrt{d}\n$$\n 现在让我们实现这个层，它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重.\n ```\n class DotProductAttention(nn.Module): \n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # query: (batch_size, #queries, d)\n    # key: (batch_size, #kv_pairs, d)\n    # value: (batch_size, #kv_pairs, dim_v)\n    # valid_length: either (batch_size, ) or (batch_size, xx)\n    def forward(self, query, key, value, valid_length=None):\n        d = query.shape[-1]\n        # set transpose_b=True to swap the last two dimensions of key\n        \n        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        print(\"attention_weight\\n\",attention_weights)\n        return torch.bmm(attention_weights, value)\n```\n\n### 多层感知机注意力\n在多层感知器中，我们首先将 query and keys 投影到  $ℝ^ℎ$ .为了更具体，我们将可以学习的参数做如下映射 \n$𝐖_𝑘∈ℝ^{ℎ×𝑑_𝑘}$ ,  $𝐖_𝑞∈ℝ^{ℎ×𝑑_𝑞}$ , and  $𝐯∈ℝ^h$ . 将score函数定义\n$$\n𝛼(𝐤,𝐪)=𝐯^𝑇tanh(𝐖_𝑘𝐤+𝐖_𝑞𝐪)\n$$\n. \n然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置.\n```\nclass MLPAttention(nn.Module):  \n    def __init__(self, units,ipt_dim,dropout, **kwargs):\n        super(MLPAttention, self).__init__(**kwargs)\n        # Use flatten=True to keep query's and key's 3-D shapes.\n        self.W_k = nn.Linear(ipt_dim, units, bias=False)\n        self.W_q = nn.Linear(ipt_dim, units, bias=False)\n        self.v = nn.Linear(units, 1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, valid_length):\n        query, key = self.W_k(query), self.W_q(key)\n        #print(\"size\",query.size(),key.size())\n        # expand query to (batch_size, #querys, 1, units), and key to\n        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n        features = query.unsqueeze(2) + key.unsqueeze(1)\n        #print(\"features:\",features.size())  #--------------开启\n        scores = self.v(features).squeeze(-1) \n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        return torch.bmm(attention_weights, value)\n```\n\n### 引入注意力机制的Seq2seq模型\n下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。在decoding阶段，解码器的$t$时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入$D_t$拼接起来一起送到解码器：\n\n![Image Name](https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800)\n\n$$\nFig1具有注意机制的seq-to-seq模型解码的第二步\n$$\n下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构\n\n![Image Name](https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800)\n$$\nFig2具有注意机制的seq-to-seq模型中层结构\n$$\n#### 解码器\n   由于带有注意机制的seq2seq的编码器不变，所以在此处我们只关注解码器。我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:\n   \n- the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values\n\n- the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state\n\n- the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）\n\n   在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。\n```\nclass Seq2SeqAttentionDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Linear(num_hiddens,vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_len, *args):\n        outputs, hidden_state = enc_outputs\n#         print(\"first:\",outputs.size(),hidden_state[0].size(),hidden_state[1].size())\n        # Transpose outputs to (batch_size, seq_len, hidden_size)\n        return (outputs.permute(1,0,-1), hidden_state, enc_valid_len)\n        #outputs.swapaxes(0, 1)\n        \n    def forward(self, X, state):\n        enc_outputs, hidden_state, enc_valid_len = state\n        #(\"X.size\",X.size())\n        X = self.embedding(X).transpose(0,1)\n#         print(\"Xembeding.size2\",X.size())\n        outputs = []\n        for l, x in enumerate(X):\n#             print(f\"\\n{l}-th token\")\n#             print(\"x.first.size()\",x.size())\n            # query shape: (batch_size, 1, hidden_size)\n            # select hidden state of the last rnn layer as query\n            query = hidden_state[0][-1].unsqueeze(1) # np.expand_dims(hidden_state[0][-1], axis=1)\n            # context has same shape as query\n#             print(\"query enc_outputs, enc_outputs:\\n\",query.size(), enc_outputs.size(), enc_outputs.size())\n            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)\n            # Concatenate on the feature dimension\n#             print(\"context.size:\",context.size())\n            x = torch.cat((context, x.unsqueeze(1)), dim=-1)\n            # Reshape x to (1, batch_size, embed_size+hidden_size)\n#             print(\"rnn\",x.size(), len(hidden_state))\n            out, hidden_state = self.rnn(x.transpose(0,1), hidden_state)\n            outputs.append(out)\n        outputs = self.dense(torch.cat(outputs, dim=0))\n        return outputs.transpose(0, 1), [enc_outputs, hidden_state,\n                                        enc_valid_len]\n```\n我们得到了相同的解码器输出形状，但是状态结构改变了。"},{"metadata":{"id":"A969F1B11EB04891A9BC2AB322873E2C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 卷积神经网络基础\n### 二维互相关运算\n二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维核（kernel）数组，输出也是一个二维数组，其中核数组通常称为卷积核或过滤器（filter）。卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。图1展示了一个互相关运算的例子，阴影部分分别是输入的第一个计算区域、核数组以及对应的输出。\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nfdbhcw5.png?imageView2/0/w/640/h/640)\n$$\n\\begin{array}{l}{0 \\times 0+1 \\times 1+3 \\times 2+4 \\times 3=19} \\\\ {1 \\times 0+2 \\times 1+4 \\times 2+5 \\times 3=25} \\\\ {3 \\times 0+4 \\times 1+6 \\times 2+7 \\times 3=37} \\\\ {4 \\times 0+5 \\times 1+7 \\times 2+8 \\times 3=43}\\end{array}\n$$\n\n```\ndef corr2d(X, K):\n    H, W = X.shape\n    h, w = K.shape\n    Y = torch.zeros(H - h + 1, W - w + 1)\n    for i in range(Y.shape[0]):\n        for j in range(Y.shape[1]):\n            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n    return Y\n```\n### 二维卷积层\n二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏置来得到输出。卷积层的模型参数包括卷积核和标量偏置。\n```\nclass Conv2D(nn.Module):\n    def __init__(self, kernel_size):\n        super(Conv2D, self).__init__()\n        self.weight = nn.Parameter(torch.randn(kernel_size))\n        self.bias = nn.Parameter(torch.randn(1))\n\n    def forward(self, x):\n        return corr2d(x, self.weight) + self.bias\n```\n### 互相关运算与卷积运算\n\n卷积层得名于卷积运算，但卷积层中用到的并非卷积运算而是互相关运算。我们将核数组上下翻转、左右翻转，再与输入数组做互相关运算，这一过程就是卷积运算。由于卷积层的核数组是可学习的，所以使用互相关运算与使用卷积运算并无本质区别。\n\n### 特征图与感受野\n\n二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素$x$的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做$x$的感受野（receptive field）。\n\n以二维互相关运算为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为$2 \\times 2$的输出记为$Y$，将$Y$与另一个形状为$2 \\times 2$的核数组做互相关运算，输出单个元素$z$。那么，$z$在$Y$上的感受野包括$Y$的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。\n\n### 填充和步幅\n#### 填充\n\n填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），图2里我们在原输入高和宽的两侧分别添加了值为0的元素。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nfl6ejy4.png?imageView2/0/w/640/h/640)\n\n\n如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，则输出形状为：\n$$\n(n_h+p_h-k_h+1)\\times(n_w+p_w-k_w+1)\n$$\n我们在卷积神经网络中使用奇数高宽的核，比如$3 \\times 3$，$5 \\times 5$的卷积核，对于高度（或宽度）为大小为$2 k + \n\n#### 步幅\n\n在互相关运算中，卷积核在输入数组上滑动，每次滑动的行数与列数即是步幅（stride）。此前我们使用的步幅都是1，下图展示了在高上步幅为3、在宽上步幅为2的二维互相关运算。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nflohnqg.png?imageView2/0/w/640/h/640)\n\n\n一般来说，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：\n\n$$\n\n\\lfloor(n_h+p_h-k_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w+p_w-k_w+s_w)/s_w\\rfloor\n\n$$\n\n\n如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \\times (n_w/s_w)$。\n\n当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。1$的核，令步幅为1，在高（或宽）两侧选择大小为$k$的填充，便可保持输入与输出尺寸相同。\n\n\n### 多输入通道和多输出通道\n\n之前的输入和输出都是二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是$h$和$w$（像素），那么它可以表示为一个$3 \\times h \\times w$的多维数组，我们将大小为3的这一维称为通道（channel）维。\n\n#### 多输入通道\n\n卷积层的输入可以包含多个通道，下图展示了一个含2个输入通道的二维互相关计算的例子。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nfmdnwbq.png?imageView2/0/w/640/h/640)\n\n假设输入数据的通道数为$c_i$，卷积核形状为$k_h\\times k_w$，我们为每个输入通道各分配一个形状为$k_h\\times k_w$的核数组，将$c_i$个互相关运算的二维输出按通道相加，得到一个二维数组作为输出。我们把$c_i$个核数组在通道维上连结，即得到一个形状为$c_i\\times k_h\\times k_w$的卷积核。\n\n#### 多输出通道\n\n卷积层的输出也可以包含多个通道，设卷积核输入通道数和输出通道数分别为$c_i$和$c_o$，高和宽分别为$k_h$和$k_w$。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为$c_i\\times k_h\\times k_w$的核数组，将它们在输出通道维上连结，卷积核的形状即$c_o\\times c_i\\times k_h\\times k_w$。\n\n对于输出通道的卷积核，我们提供这样一种理解，一个$c_i \\times k_h \\times k_w$的核数组可以提取某种局部特征，但是输入可能具有相当丰富的特征，我们需要有多个这样的$c_i \\times k_h \\times k_w$的核数组，不同的核数组提取的是不同的特征。\n\n### 卷积层与全连接层的对比\n二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：\n\n一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。\n\n二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为$(c_i, c_o, h, w)$的卷积核的参数量是$c_i \\times c_o \\times h \\times w$，与输入图像的宽高无关。假如一个卷积层的输入和输出形状分别是$(c_1, h_1, w_1)$和$(c_2, h_2, w_2)$，如果要用全连接层进行连接，参数数量就是$c_1 \\times c_2 \\times h_1 \\times w_1 \\times h_2 \\times w_2$。使用卷积层可以以较少的参数数量来处理更大的图像。\n\n### 池化\n\n#### 二维池化层\n\n池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做**最大池化或平均池化**。下图展示了池化窗口形状为$2\\times 2$的最大池化。\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5nfob3odo.png?imageView2/0/w/640/h/640)\n$$\n\\begin{array}{l}{\\max (0,1,3,4)=4} \\\\ {\\max (1,2,4,5)=5} \\\\ {\\max (3,4,6,7)=7} \\\\ {\\max (4,5,7,8)=8}\\end{array}\n$$\n二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为$p \\times q$的池化层称为$p \\times q$池化层，其中的池化运算叫作$p \\times q$池化。\n\n池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。\n\n在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。\n```\nX = torch.arange(32, dtype=torch.float32).view(1, 2, 4, 4)\npool2d = nn.MaxPool2d(kernel_size=3, padding=1, stride=(2, 1))\nY = pool2d(X)\n```\n平均池化层使用的是`nn.AvgPool2d`，使用方法与`nn.MaxPool2d`相同。"},{"metadata":{"id":"92245AE2951B4894BA5BE0C4D6AA4E4F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## LeNet 模型\nLeNet分为卷积层块和全连接层块两个部分\n\n卷积层块里的基本单位是卷积层后接平均池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的平均池化层则用来降低卷积层对位置的敏感性。\n\n卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用$5 \\times 5$的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。\n\n全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。\n\n```\n#net\nclass Flatten(torch.nn.Module):  #展平操作\n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n\nclass Reshape(torch.nn.Module): #将图像大小重定型\n    def forward(self, x):\n        return x.view(-1,1,28,28)      #(B x C x H x W)\n    \nnet = torch.nn.Sequential(     #Lelet                                                  \n    Reshape(),\n    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #b*1*28*28  =>b*6*28*28\n    nn.Sigmoid(),                                                       \n    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*6*28*28  =>b*6*14*14\n    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),           #b*6*14*14  =>b*16*10*10\n    nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*16*10*10  => b*16*5*5\n    Flatten(),                                                          #b*16*5*5   => b*400\n    nn.Linear(in_features=16*5*5, out_features=120),\n    nn.Sigmoid(),\n    nn.Linear(120, 84),\n    nn.Sigmoid(),\n    nn.Linear(84, 10)\n)\n#print\nX = torch.randn(size=(1,1,28,28), dtype = torch.float32)\nfor layer in net:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape: \\t',X.shape)\n'''\nReshape output shape: \t torch.Size([1, 1, 28, 28])\nConv2d output shape: \t torch.Size([1, 6, 28, 28])\nSigmoid output shape: \t torch.Size([1, 6, 28, 28])\nAvgPool2d output shape: \t torch.Size([1, 6, 14, 14])\nConv2d output shape: \t torch.Size([1, 16, 10, 10])\nSigmoid output shape: \t torch.Size([1, 16, 10, 10])\nAvgPool2d output shape: \t torch.Size([1, 16, 5, 5])\nFlatten output shape: \t torch.Size([1, 400])\nLinear output shape: \t torch.Size([1, 120])\nSigmoid output shape: \t torch.Size([1, 120])\nLinear output shape: \t torch.Size([1, 84])\nSigmoid output shape: \t torch.Size([1, 84])\nLinear output shape: \t torch.Size([1, 10])\n'''\n```\n\n卷积层由于使用高和宽均为5的卷积核，从而将高和宽分别减小4，而池化层则将高和宽减半，但通道数则从1增加到16。全连接层则逐层减少输出个数，直到变成图像的类别数10。\n![Image Name](https://cdn.kesci.com/upload/image/q5ndxi6jl5.png?imageView2/0/w/640/h/640)\n\n\n\n卷积神经网络就是含卷积层的网络。\nLeNet交替使用卷积层和最大池化层后接全连接层来进行图像分类。"},{"metadata":{"id":"0CCD1DE2C14547F6A8039BC034FE4994","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 深度卷积神经网络（AlexNet）\n\nLeNet:  在大的真实数据集上的表现并不尽如⼈意。     \n1.神经网络计算复杂。  \n2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。  \n  \n机器学习的特征提取:手工定义的特征提取函数  \n神经网络的特征提取：通过学习得到数据的多级表征，并逐级表⽰越来越抽象的概念或模式。  \n  \n神经网络发展的限制:数据、硬件\n\n### AlexNet\n首次证明了学习到的特征可以超越⼿⼯设计的特征，从而⼀举打破计算机视觉研究的前状。\n**特征：**\n1. 8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。\n2. 将sigmoid激活函数改成了更加简单的ReLU激活函数。\n3. 用Dropout来控制全连接层的模型复杂度。\n4. 引入数据增强，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。\n\n![Image Name](https://cdn.kesci.com/upload/image/q5kv4gpx88.png?imageView2/0/w/640/h/640)\n\n```\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2), # kernel_size, stride\n            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n            nn.Conv2d(96, 256, 5, 1, 2),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2),\n            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n            # 前两个卷积层后不使用池化层来减小输入的高和宽\n            nn.Conv2d(256, 384, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(384, 384, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(384, 256, 3, 1, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2)\n        )\n         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n        self.fc = nn.Sequential(\n            nn.Linear(256*5*5, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            #由于使用CPU镜像，精简网络，若为GPU镜像可添加该层\n            #nn.Linear(4096, 4096),\n            #nn.ReLU(),\n            #nn.Dropout(0.5),\n\n            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n            nn.Linear(4096, 10),\n        )\n\n    def forward(self, img):\n\n        feature = self.conv(img)\n        output = self.fc(feature.view(img.shape[0], -1))\n        return output\n```\n\n### VGG使用重复元素的网络\nVGG：通过重复使⽤简单的基础块来构建深度模型。  \nBlock:数个相同的填充为1、窗口形状为$3\\times 3$的卷积层,接上一个步幅为2、窗口形状为$2\\times 2$的最大池化层。  \n卷积层保持输入的高和宽不变，而池化层则对其减半。\n\n![Image Name](https://cdn.kesci.com/upload/image/q5l6vut7h1.png?imageView2/0/w/640/h/640)\n\n```\ndef vgg_block(num_convs, in_channels, out_channels): #卷积层个数，输入通道数，输出通道数\n    blk = []\n    for i in range(num_convs):\n        if i == 0:\n            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n        else:\n            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n        blk.append(nn.ReLU())\n    blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # 这里会使宽高减半\n    return nn.Sequential(*blk)\n\ndef vgg(conv_arch, fc_features, fc_hidden_units=4096):\n    net = nn.Sequential()\n    # 卷积层部分\n    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):\n        # 每经过一个vgg_block都会使宽高减半\n        net.add_module(\"vgg_block_\" + str(i+1), vgg_block(num_convs, in_channels, out_channels))\n    # 全连接层部分\n    net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(),\n                                 nn.Linear(fc_features, fc_hidden_units),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.5),\n                                 nn.Linear(fc_hidden_units, fc_hidden_units),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.5),\n                                 nn.Linear(fc_hidden_units, 10)\n                                ))\n    return net\n\t\t\n```\n###  ⽹络中的⽹络（NiN） \nLeNet、AlexNet和VGG：先以由卷积层构成的模块充分抽取 空间特征，再以由全连接层构成的模块来输出分类结果。  \nNiN：串联多个由卷积层和“全连接”层构成的小⽹络来构建⼀个深层⽹络。  \n⽤了输出通道数等于标签类别数的NiN块，然后使⽤全局平均池化层对每个通道中所有元素求平均并直接⽤于分类。  \n\n![Image Name](https://cdn.kesci.com/upload/image/q5l6u1p5vy.png?imageView2/0/w/960/h/960)\n\n1×1卷积核作用   \n1.放缩通道数：通过控制卷积核的数量达到通道数的放缩。  \n2.增加非线性。1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。  \n3.计算参数少\n\nNiN重复使⽤由卷积层和代替全连接层的1×1卷积层构成的NiN块来构建深层⽹络。  \nNiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数 的NiN块和全局平均池化层。   \nNiN的以上设计思想影响了后⾯⼀系列卷积神经⽹络的设计。  \n\n\n```\ndef nin_block(in_channels, out_channels, kernel_size, stride, padding):\n    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n                        nn.ReLU(),\n                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n                        nn.ReLU(),\n                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n                        nn.ReLU())\n    return blk\nclass GlobalAvgPool2d(nn.Module):\n    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n    def forward(self, x):\n        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n\nnet = nn.Sequential(\n    nin_block(1, 96, kernel_size=11, stride=4, padding=0),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n    nn.MaxPool2d(kernel_size=3, stride=2), \n    nn.Dropout(0.5),\n    # 标签类别数是10\n    nin_block(384, 10, kernel_size=3, stride=1, padding=1),\n    GlobalAvgPool2d(), \n    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n    d2l.FlattenLayer())\nX = torch.rand(1, 1, 224, 224)\nfor name, blk in net.named_children(): \n    X = blk(X)\n    print(name, 'output shape: ', X.shape)\n'''\n0 output shape:  torch.Size([1, 96, 54, 54])\n1 output shape:  torch.Size([1, 96, 26, 26])\n2 output shape:  torch.Size([1, 256, 26, 26])\n3 output shape:  torch.Size([1, 256, 12, 12])\n4 output shape:  torch.Size([1, 384, 12, 12])\n5 output shape:  torch.Size([1, 384, 5, 5])\n6 output shape:  torch.Size([1, 384, 5, 5])\n7 output shape:  torch.Size([1, 10, 5, 5])\n8 output shape:  torch.Size([1, 10, 1, 1])\n9 output shape:  torch.Size([1, 10])\n'''\n```\n\n\n#### GoogLeNet :Networks with Parallel Concatenations\n1. 由Inception基础块组成。  \n2. Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层来并⾏抽取信息，并使⽤1×1卷积层减少通道数从而降低模型复杂度。   \n3. 可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。 \n\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5weakogtb.png?imageView2/0/w/640/h/640)\n\n\n\n完整模型结构  \n\n![Image Name](https://cdn.kesci.com/upload/image/q5l6x0fyyn.png?imageView2/0/w/640/h/640)\n\n\n```\nclass Inception(nn.Module):\n    # c1 - c4为每条线路里的层的输出通道数\n    def __init__(self, in_c, c1, c2, c3, c4):\n        super(Inception, self).__init__()\n        # 线路1，单1 x 1卷积层\n        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)\n        # 线路2，1 x 1卷积层后接3 x 3卷积层\n        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # 线路3，1 x 1卷积层后接5 x 5卷积层\n        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)\n\n    def forward(self, x):\n        p1 = F.relu(self.p1_1(x))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n        p4 = F.relu(self.p4_2(self.p4_1(x)))\n        return torch.cat((p1, p2, p3, p4), dim=1)  # 在通道维上连结输出\n```"},{"metadata":{"id":"0B3BF201030040DB82796568250E7F37","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"code","source":"参考：\nhttps://zhuanlan.zhihu.com/p/32481747\nhttps://zhuanlan.zhihu.com/p/32085405","execution_count":null,"outputs":[]},{"metadata":{"id":"1AF7CEA004554AFCAD84250772D8DF50","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}