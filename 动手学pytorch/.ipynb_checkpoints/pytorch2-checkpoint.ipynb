{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F68298420C154AE5A017CF5BDF69310C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 模型选择、过拟合和欠拟合\n",
    "- **模型选择**\n",
    "\t1.**验证数据集**\n",
    "\t从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。\n",
    "\t2.**K折交叉验证**   \n",
    "\t由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。\n",
    "\t\n",
    "\n",
    "- **过拟合和欠拟合** \n",
    "\t模型训练中经常出现的两类典型问题：\n",
    "一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；\n",
    "另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。\n",
    "在实践中，我们要尽可能同时应对欠拟合和过拟合。\n",
    "\t1.**模型复杂度**\n",
    "\t给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数\n",
    "$$\n",
    " \\hat{y} = b + \\sum_{k=1}^K x^k w_k \n",
    "$$\n",
    "来近似 $y$。在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。\n",
    "给定训练数据集，模型复杂度和误差之间的关系：\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jc27wxoj.png?imageView2/0/w/960/h/960)\n",
    "\t2.**训练数据集大小**\n",
    "\t影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。\n",
    "\t\n",
    "- **解决方法**\n",
    "\t1.**正则化**\n",
    "\t通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。\n",
    "\t$L_2$范数正则化令**权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度**。因此，$L_2$范数正则化又叫**权重衰减**。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。\n",
    "\t2.**丢弃法**\n",
    "\t多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \\ldots, 5$）的计算表达式为\n",
    "$$\n",
    " h_i = \\phi\\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\\right) \n",
    "$$\n",
    "这里$\\phi$是激活函数，$x_1, \\ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \\ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i'$\n",
    "$$\n",
    " h_i' = \\frac{\\xi_i}{1-p} h_i \n",
    "$$\n",
    "由于$E(\\xi_i) = 1-p$，因此\n",
    "$$\n",
    " E(h_i') = \\frac{E(\\xi_i)}{1-p}h_i = h_i \n",
    "$$\n",
    "即丢弃法不改变其输入的期望值。让我们对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \\ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \\ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jd69in3m.png?imageView2/0/w/960/h/960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CD1A38B37FB2489C9B5E7C5240A11AEB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    X = X.float()\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape) < keep_prob).float()\n",
    "    \n",
    "    return mask * X / keep_prob\n",
    "    \n",
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "def net(X, is_training=True):\n",
    "    X = X.view(-1, num_inputs)\n",
    "    H1 = (torch.matmul(X, W1) + b1).relu()\n",
    "    if is_training:  # 只在训练模型时使用丢弃法\n",
    "        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层\n",
    "    H2 = (torch.matmul(H1, W2) + b2).relu()\n",
    "    if is_training:\n",
    "        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层\n",
    "    return torch.matmul(H2, W3) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_type": "code",
    "id": "509C94777E2E416D86BC6AD4DAEE8DBA",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 梯度消失和梯度爆炸\n",
    "深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。\n",
    "- **当神经网络的层数较多时，模型的数值稳定性容易变差。**\n",
    "\t假设一个层数为$L$的多层感知机的第$l$层$\\boldsymbol{H}^{(l)}$的权重参数为$\\boldsymbol{W}^{(l)}$，输出层$\\boldsymbol{H}^{(L)}$的权重参数为$\\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\\phi(x) = x$。给定输入$\\boldsymbol{X}$，多层感知机的第$l$层的输出$\\boldsymbol{H}^{(l)} = \\boldsymbol{X} \\boldsymbol{W}^{(1)} \\boldsymbol{W}^{(2)} \\ldots \\boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\\boldsymbol{X}$分别与$0.2^{30} \\approx 1 \\times 10^{-21}$（消失）和$5^{30} \\approx 9 \\times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。\n",
    "\t\n",
    "- **随机初始化模型参数的原因**\n",
    "\t假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。\n",
    "\t1. PyTorch的默认随机初始化 \n",
    "\t 在线性回归的简洁实现中，我们使用`torch.nn.init.normal_()`使模型`net`的权重参数采用正态分布的随机初始化方式。不过，PyTorch中`nn.Module`的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考[源代码](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules)），因此一般不用我们考虑\n",
    "\t2. Xavier随机初始化\n",
    "\t假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布\n",
    "$$\n",
    "U\\left(-\\sqrt{\\frac{6}{a+b}}, \\sqrt{\\frac{6}{a+b}}\\right).\n",
    "$$\n",
    "它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3C74B45AD414C969E95EB6A345A2EB0",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 循环神经网络进阶\n",
    "\n",
    "### GRU\n",
    "RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）  \n",
    "GRU⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系 ,解决长期记忆和反向传播中的梯度等问题。\n",
    "• 重置⻔有助于捕捉时间序列⾥短期的依赖关系；  \n",
    "• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。 \n",
    "\n",
    "- **GRU的输入输出结构**\n",
    "GRU的输入输出结构与普通的RNN是一样的。\n",
    "有一个当前的输入$x^t$ ，和上一个节点传递下来的隐状态（hidden state）$h^{t-1}$，这个隐状态包含了之前节点的相关信息。\n",
    "结合$x^t$ 和$h^{t-1}$，GRU会得到当前隐藏节点的输出$y^t$ 和传递给下一个节点的隐状态$h^t$。\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5srwnnmv7.png?imageView2/0/w/360/h/360)\n",
    "\n",
    "- **GRU的内部结构**\n",
    "首先，我们先通过上一个传输下来的状态 $h^{t-1}$ 和当前节点的输入$x^t$ 来获取两个门控状态。如下图所示，其中$r$控制重置的门控（reset gate），$z$为控制更新的门控（update gate）。\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5ss7lod3r.png?imageView2/0/w/360/h/360)\n",
    "\n",
    "得到门控信号之后，首先使用重置门控来得到“重置”之后的数据$h^{t-1'}=h^{t-1}⊙r$ ，再将$h^{t-1'}$与输入 $x^r$ 进行拼接，再通过一个tanh激活函数来将数据放缩到-1~1的范围内。即得到如下图所示的$h'$ 。\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5ssjfuxhf.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "这里的$h'$主要是包含了当前输入的$x^t$数据。有针对性地对$h'$添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5ssnlsje7.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "最后介绍GRU最关键的一个步骤**更新记忆**阶段。\n",
    "\n",
    "在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控$z$（update gate）。\n",
    "更新表达式：$h^t = z ⊙h^{t-1}+(1-z)⊙h^{'}$\n",
    "\n",
    "首先再次强调一下，门控信号（z）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。\n",
    "\n",
    "GRU很聪明的一点就在于，我们使用了同一个门控$z$就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。\n",
    "\n",
    "**$z ⊙h^{t-1}$**：表示对原本隐藏状态的选择性“遗忘”。这里的$z$可以想象成遗忘门（forget gate），忘记$h^{t-1}$维度中一些不重要的信息。\n",
    "\n",
    "**$(1-z)⊙h'$** ： 表示对包含当前节点信息的$h'$进行选择性”记忆“。与上面类似，这里的$(1-z)$同理会忘记$h'$ 维度中的一些不重要的信息。或者，这里我们更应当看做是对h'$维度中的某些信息进行选择。\n",
    "\n",
    "**$h^t = z ⊙h^{t-1}+(1-z)⊙h^{'}$**:结合上述，这一步的操作就是忘记传递下来的 $h^{t-1}$中的某些维度信息，并加入当前节点输入的某些维度信息。\n",
    "可以看到，这里的遗忘$z$和选择$(1-z)$是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （z），我们就会使用包含当前输入的$h'$中所对应的权重进行弥补(1-z)。以保持一种”恒定“状态。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EC1D378FA5B6461F889253B7F7C9DE1C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##初始化参数\n",
    "num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\n",
    "print('will use', device)\n",
    "\n",
    "def get_params():  \n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32) #正态分布\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "    def _three():\n",
    "        return (_one((num_inputs, num_hiddens)),\n",
    "                _one((num_hiddens, num_hiddens)),\n",
    "                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))\n",
    "     \n",
    "    W_xz, W_hz, b_z = _three()  # 更新门参数\n",
    "    W_xr, W_hr, b_r = _three()  # 重置门参数\n",
    "    W_xh, W_hh, b_h = _three()  # 候选隐藏状态参数\n",
    "    \n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)\n",
    "    return nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])\n",
    "\n",
    "def init_gru_state(batch_size, num_hiddens, device):   #隐藏状态初始化\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "    \n",
    "##GRU模型\n",
    "def gru(inputs, state, params):\n",
    "    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)\n",
    "        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)\n",
    "        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilda\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E4B327C87A14B5D8FF1D8A36DE57241",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### LSTM\n",
    "长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。\n",
    "\n",
    "遗忘门:控制上一时间步的记忆细胞 \n",
    "输入门:控制当前时间步的输入  \n",
    "输出门:控制从记忆细胞到隐藏状态  \n",
    "记忆细胞：⼀种特殊的隐藏状态的信息的流动 \n",
    "\n",
    "- **LSTM结构**\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5stl6xcdz.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5stmqibvz.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。\n",
    "\n",
    "但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "40BE5EC1E44E4C7293C5E188B5AA8500",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##初始化参数、隐层、模型定义\n",
    "def get_params():\n",
    "    def _one(shape):\n",
    "        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n",
    "        return torch.nn.Parameter(ts, requires_grad=True)\n",
    "    def _three():\n",
    "        return (_one((num_inputs, num_hiddens)),\n",
    "                _one((num_hiddens, num_hiddens)),\n",
    "                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))\n",
    "          \n",
    "    W_xi, W_hi, b_i = _three()  # 输入门参数\n",
    "    W_xf, W_hf, b_f = _three()  # 遗忘门参数\n",
    "    W_xo, W_ho, b_o = _three()  # 输出门参数\n",
    "    W_xc, W_hc, b_c = _three()  # 候选记忆细胞参数\n",
    "          \n",
    "    # 输出层参数\n",
    "    W_hq = _one((num_hiddens, num_outputs))\n",
    "    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)\n",
    "    return nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])\n",
    "          \n",
    "          \n",
    "def init_lstm_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), \n",
    "            torch.zeros((batch_size, num_hiddens), device=device))\n",
    "                  \n",
    "      \n",
    "def lstm(inputs, state, params):\n",
    "    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params\n",
    "    (H, C) = state\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)\n",
    "        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)\n",
    "        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)\n",
    "        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)\n",
    "        C = F * C + I * C_tilda\n",
    "        H = O * C.tanh()\n",
    "        Y = torch.matmul(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return outputs, (H, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CA727FD0E904AA39A1C52F018BD37BF",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 深度循环神经网络  \n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5jk3z1hvz.png?imageView2/0/w/320/h/320)\n",
    "\n",
    "$$\n",
    "\\boldsymbol{H}_t^{(1)} = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(1)} + \\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{hh}^{(1)} + \\boldsymbol{b}_h^{(1)})\\\\\n",
    "\\boldsymbol{H}_t^{(\\ell)} = \\phi(\\boldsymbol{H}_t^{(\\ell-1)} \\boldsymbol{W}_{xh}^{(\\ell)} + \\boldsymbol{H}_{t-1}^{(\\ell)} \\boldsymbol{W}_{hh}^{(\\ell)} + \\boldsymbol{b}_h^{(\\ell)})\\\\\n",
    "\\boldsymbol{O}_t = \\boldsymbol{H}_t^{(L)} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D49649152A84435CB8057457658FA456",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 双向循环神经网络 \n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q5j8hmgyrz.png?imageView2/0/w/320/h/320)\n",
    "\n",
    "$$ \n",
    "\\begin{aligned} \\overrightarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(f)} + \\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{hh}^{(f)} + \\boldsymbol{b}_h^{(f)})\\\\\n",
    "\\overleftarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(b)} + \\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{hh}^{(b)} + \\boldsymbol{b}_h^{(b)}) \\end{aligned} $$\n",
    "$$\n",
    "\\boldsymbol{H}_t=(\\overrightarrow{\\boldsymbol{H}}_{t}, \\overleftarrow{\\boldsymbol{H}}_t)\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CC05C1EB07A41CC9C0A884549B51AC0",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46957AA95325436E97DD9446BB181F5D",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A969F1B11EB04891A9BC2AB322873E2C",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "92245AE2951B4894BA5BE0C4D6AA4E4F",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0B3BF201030040DB82796568250E7F37",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "参考：\n",
    "https://zhuanlan.zhihu.com/p/32481747\n",
    "https://zhuanlan.zhihu.com/p/32085405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AF7CEA004554AFCAD84250772D8DF50",
    "jupyter": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
