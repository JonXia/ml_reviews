{"cells":[{"metadata":{"id":"F68298420C154AE5A017CF5BDF69310C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 模型选择、过拟合和欠拟合\n- **模型选择**\n\t1.**验证数据集**\n\t从严格意义上讲，测试集只能在所有超参数和模型参数选定后使用一次。不可以使用测试数据选择模型，如调参。由于无法从训练误差估计泛化误差，因此也不应只依赖训练数据选择模型。鉴于此，我们可以预留一部分在训练数据集和测试数据集以外的数据来进行模型选择。这部分数据被称为验证数据集，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取一小部分作为验证集，而将剩余部分作为真正的训练集。\n\t2.**K折交叉验证**   \n\t由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。\n\t\n\n- **过拟合和欠拟合** \n\t模型训练中经常出现的两类典型问题：\n一类是模型无法得到较低的训练误差，我们将这一现象称作欠拟合（underfitting）；\n另一类是模型的训练误差远小于它在测试数据集上的误差，我们称该现象为过拟合（overfitting）。\n在实践中，我们要尽可能同时应对欠拟合和过拟合。\n\t1.**模型复杂度**\n\t给定一个由标量数据特征$x$和对应的标量标签$y$组成的训练数据集，多项式函数拟合的目标是找一个$K$阶多项式函数\n$$\n \\hat{y} = b + \\sum_{k=1}^K x^k w_k \n$$\n来近似 $y$。在上式中，$w_k$是模型的权重参数，$b$是偏差参数。与线性回归相同，多项式函数拟合也使用平方损失函数。特别地，一阶多项式函数拟合又叫线性函数拟合。\n给定训练数据集，模型复杂度和误差之间的关系：\n![Image Name](https://cdn.kesci.com/upload/image/q5jc27wxoj.png?imageView2/0/w/960/h/960)\n\t2.**训练数据集大小**\n\t影响欠拟合和过拟合的另一个重要因素是训练数据集的大小。一般来说，如果训练数据集中样本数过少，特别是比模型参数数量（按元素计）更少时，过拟合更容易发生。此外，泛化误差不会随训练数据集里样本数量增加而增大。因此，在计算资源允许的范围之内，我们通常希望训练数据集大一些，特别是在模型复杂度较高时，例如层数较多的深度学习模型。\n\t\n- **解决方法**\n\t1.**正则化**\n\t通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。\n\t$L_2$范数正则化令**权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度**。因此，$L_2$范数正则化又叫**权重衰减**。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。\n\t2.**丢弃法**\n\t多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \\ldots, 5$）的计算表达式为\n$$\n h_i = \\phi\\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\\right) \n$$\n这里$\\phi$是激活函数，$x_1, \\ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \\ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i'$\n$$\n h_i' = \\frac{\\xi_i}{1-p} h_i \n$$\n由于$E(\\xi_i) = 1-p$，因此\n$$\n E(h_i') = \\frac{E(\\xi_i)}{1-p}h_i = h_i \n$$\n即丢弃法不改变其输入的期望值。让我们对之前多层感知机的神经网络中的隐藏层使用丢弃法，一种可能的结果如图所示，其中$h_2$和$h_5$被清零。这时输出值的计算不再依赖$h_2$和$h_5$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1, \\ldots, h_5$都有可能被清零，输出层的计算无法过度依赖$h_1, \\ldots, h_5$中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法\n![Image Name](https://cdn.kesci.com/upload/image/q5jd69in3m.png?imageView2/0/w/960/h/960)"},{"metadata":{"id":"CD1A38B37FB2489C9B5E7C5240A11AEB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def dropout(X, drop_prob):\n    X = X.float()\n    assert 0 <= drop_prob <= 1\n    keep_prob = 1 - drop_prob\n    # 这种情况下把全部元素都丢弃\n    if keep_prob == 0:\n        return torch.zeros_like(X)\n    mask = (torch.rand(X.shape) < keep_prob).float()\n    \n    return mask * X / keep_prob\n    \ndrop_prob1, drop_prob2 = 0.2, 0.5\ndef net(X, is_training=True):\n    X = X.view(-1, num_inputs)\n    H1 = (torch.matmul(X, W1) + b1).relu()\n    if is_training:  # 只在训练模型时使用丢弃法\n        H1 = dropout(H1, drop_prob1)  # 在第一层全连接后添加丢弃层\n    H2 = (torch.matmul(H1, W2) + b2).relu()\n    if is_training:\n        H2 = dropout(H2, drop_prob2)  # 在第二层全连接后添加丢弃层\n    return torch.matmul(H2, W3) + b3","execution_count":1},{"metadata":{"cell_type":"code","id":"509C94777E2E416D86BC6AD4DAEE8DBA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 梯度消失和梯度爆炸\n深度模型有关数值稳定性的典型问题是消失（vanishing）和爆炸（explosion）。\n- **当神经网络的层数较多时，模型的数值稳定性容易变差。**\n\t假设一个层数为$L$的多层感知机的第$l$层$\\boldsymbol{H}^{(l)}$的权重参数为$\\boldsymbol{W}^{(l)}$，输出层$\\boldsymbol{H}^{(L)}$的权重参数为$\\boldsymbol{W}^{(L)}$。为了便于讨论，不考虑偏差参数，且设所有隐藏层的激活函数为恒等映射（identity mapping）$\\phi(x) = x$。给定输入$\\boldsymbol{X}$，多层感知机的第$l$层的输出$\\boldsymbol{H}^{(l)} = \\boldsymbol{X} \\boldsymbol{W}^{(1)} \\boldsymbol{W}^{(2)} \\ldots \\boldsymbol{W}^{(l)}$。此时，如果层数$l$较大，$\\boldsymbol{H}^{(l)}$的计算可能会出现衰减或爆炸。举个例子，假设输入和所有层的权重参数都是标量，如权重参数为0.2和5，多层感知机的第30层输出为输入$\\boldsymbol{X}$分别与$0.2^{30} \\approx 1 \\times 10^{-21}$（消失）和$5^{30} \\approx 9 \\times 10^{20}$（爆炸）的乘积。当层数较多时，梯度的计算也容易出现消失或爆炸。\n\t\n- **随机初始化模型参数的原因**\n\t假设输出层只保留一个输出单元$o_1$（删去$o_2$和$o_3$以及指向它们的箭头），且隐藏层使用相同的激活函数。如果将每个隐藏单元的参数都初始化为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层。在反向传播中，每个隐藏单元的参数梯度值相等。因此，这些参数在使用基于梯度的优化算法迭代后值依然相等。之后的迭代也是如此。在这种情况下，无论隐藏单元有多少，隐藏层本质上只有1个隐藏单元在发挥作用。因此，正如在前面的实验中所做的那样，我们通常将神经网络的模型参数，特别是权重参数，进行随机初始化。\n\t1. PyTorch的默认随机初始化 \n\t 在线性回归的简洁实现中，我们使用`torch.nn.init.normal_()`使模型`net`的权重参数采用正态分布的随机初始化方式。不过，PyTorch中`nn.Module`的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法的可参考[源代码](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules)），因此一般不用我们考虑\n\t2. Xavier随机初始化\n\t假设某全连接层的输入个数为$a$，输出个数为$b$，Xavier随机初始化将使该层中权重参数的每个元素都随机采样于均匀分布\n$$\nU\\left(-\\sqrt{\\frac{6}{a+b}}, \\sqrt{\\frac{6}{a+b}}\\right).\n$$\n它的设计主要考虑到，模型参数初始化后，每层输出的方差不该受该层输入个数影响，且每层梯度的方差也不该受该层输出个数影响。"},{"metadata":{"id":"A3C74B45AD414C969E95EB6A345A2EB0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 循环神经网络进阶\n\n### GRU\nRNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）  \nGRU⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系 ,解决长期记忆和反向传播中的梯度等问题。\n• 重置⻔有助于捕捉时间序列⾥短期的依赖关系；  \n• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。 \n\n- **GRU的输入输出结构**\nGRU的输入输出结构与普通的RNN是一样的。\n有一个当前的输入$x^t$ ，和上一个节点传递下来的隐状态（hidden state）$h^{t-1}$，这个隐状态包含了之前节点的相关信息。\n结合$x^t$ 和$h^{t-1}$，GRU会得到当前隐藏节点的输出$y^t$ 和传递给下一个节点的隐状态$h^t$。\n![Image Name](https://cdn.kesci.com/upload/image/q5srwnnmv7.png?imageView2/0/w/360/h/360)\n\n- **GRU的内部结构**\n首先，我们先通过上一个传输下来的状态 $h^{t-1}$ 和当前节点的输入$x^t$ 来获取两个门控状态。如下图所示，其中$r$控制重置的门控（reset gate），$z$为控制更新的门控（update gate）。\n![Image Name](https://cdn.kesci.com/upload/image/q5ss7lod3r.png?imageView2/0/w/360/h/360)\n\n得到门控信号之后，首先使用重置门控来得到“重置”之后的数据$h^{t-1'}=h^{t-1}⊙r$ ，再将$h^{t-1'}$与输入 $x^r$ 进行拼接，再通过一个tanh激活函数来将数据放缩到-1~1的范围内。即得到如下图所示的$h'$ 。\n![Image Name](https://cdn.kesci.com/upload/image/q5ssjfuxhf.png?imageView2/0/w/960/h/960)\n\n这里的$h'$主要是包含了当前输入的$x^t$数据。有针对性地对$h'$添加到当前的隐藏状态，相当于”记忆了当前时刻的状态“\n![Image Name](https://cdn.kesci.com/upload/image/q5ssnlsje7.png?imageView2/0/w/960/h/960)\n\n最后介绍GRU最关键的一个步骤**更新记忆**阶段。\n\n在这个阶段，我们同时进行了遗忘了记忆两个步骤。我们使用了先前得到的更新门控$z$（update gate）。\n更新表达式：$h^t = z ⊙h^{t-1}+(1-z)⊙h^{'}$\n\n首先再次强调一下，门控信号（z）的范围为0~1。门控信号越接近1，代表”记忆“下来的数据越多；而越接近0则代表”遗忘“的越多。\n\nGRU很聪明的一点就在于，我们使用了同一个门控$z$就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。\n\n**$z ⊙h^{t-1}$**：表示对原本隐藏状态的选择性“遗忘”。这里的$z$可以想象成遗忘门（forget gate），忘记$h^{t-1}$维度中一些不重要的信息。\n\n**$(1-z)⊙h'$** ： 表示对包含当前节点信息的$h'$进行选择性”记忆“。与上面类似，这里的$(1-z)$同理会忘记$h'$ 维度中的一些不重要的信息。或者，这里我们更应当看做是对h'$维度中的某些信息进行选择。\n\n**$h^t = z ⊙h^{t-1}+(1-z)⊙h^{'}$**:结合上述，这一步的操作就是忘记传递下来的 $h^{t-1}$中的某些维度信息，并加入当前节点输入的某些维度信息。\n可以看到，这里的遗忘$z$和选择$(1-z)$是联动的。也就是说，对于传递进来的维度信息，我们会进行选择性遗忘，则遗忘了多少权重 （z），我们就会使用包含当前输入的$h'$中所对应的权重进行弥补(1-z)。以保持一种”恒定“状态。\n\n"},{"metadata":{"id":"EC1D378FA5B6461F889253B7F7C9DE1C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"##初始化参数\nnum_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size\nprint('will use', device)\n\ndef get_params():  \n    def _one(shape):\n        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32) #正态分布\n        return torch.nn.Parameter(ts, requires_grad=True)\n    def _three():\n        return (_one((num_inputs, num_hiddens)),\n                _one((num_hiddens, num_hiddens)),\n                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))\n     \n    W_xz, W_hz, b_z = _three()  # 更新门参数\n    W_xr, W_hr, b_r = _three()  # 重置门参数\n    W_xh, W_hh, b_h = _three()  # 候选隐藏状态参数\n    \n    # 输出层参数\n    W_hq = _one((num_hiddens, num_outputs))\n    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)\n    return nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q])\n\ndef init_gru_state(batch_size, num_hiddens, device):   #隐藏状态初始化\n    return (torch.zeros((batch_size, num_hiddens), device=device), )\n    \n##GRU模型\ndef gru(inputs, state, params):\n    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params\n    H, = state\n    outputs = []\n    for X in inputs:\n        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)\n        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)\n        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)\n        H = Z * H + (1 - Z) * H_tilda\n        Y = torch.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return outputs, (H,)","execution_count":2},{"metadata":{"id":"7E4B327C87A14B5D8FF1D8A36DE57241","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### LSTM\n长短期记忆（Long short-term memory, LSTM）是一种特殊的RNN，主要是为了解决长序列训练过程中的梯度消失和梯度爆炸问题。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。\n\n遗忘门:控制上一时间步的记忆细胞 \n输入门:控制当前时间步的输入  \n输出门:控制从记忆细胞到隐藏状态  \n记忆细胞：⼀种特殊的隐藏状态的信息的流动 \n\n- **LSTM结构**\n\n![Image Name](https://cdn.kesci.com/upload/image/q5stl6xcdz.png?imageView2/0/w/960/h/960)\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q5stmqibvz.png?imageView2/0/w/960/h/960)\n\n以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。\n\n但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。"},{"metadata":{"id":"40BE5EC1E44E4C7293C5E188B5AA8500","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"##初始化参数、隐层、模型定义\ndef get_params():\n    def _one(shape):\n        ts = torch.tensor(np.random.normal(0, 0.01, size=shape), device=device, dtype=torch.float32)\n        return torch.nn.Parameter(ts, requires_grad=True)\n    def _three():\n        return (_one((num_inputs, num_hiddens)),\n                _one((num_hiddens, num_hiddens)),\n                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=True))\n          \n    W_xi, W_hi, b_i = _three()  # 输入门参数\n    W_xf, W_hf, b_f = _three()  # 遗忘门参数\n    W_xo, W_ho, b_o = _three()  # 输出门参数\n    W_xc, W_hc, b_c = _three()  # 候选记忆细胞参数\n          \n    # 输出层参数\n    W_hq = _one((num_hiddens, num_outputs))\n    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=True)\n    return nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])\n          \n          \ndef init_lstm_state(batch_size, num_hiddens, device):\n    return (torch.zeros((batch_size, num_hiddens), device=device), \n            torch.zeros((batch_size, num_hiddens), device=device))\n                  \n      \ndef lstm(inputs, state, params):\n    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params\n    (H, C) = state\n    outputs = []\n    for X in inputs:\n        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)\n        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)\n        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)\n        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)\n        C = F * C + I * C_tilda\n        H = O * C.tanh()\n        Y = torch.matmul(H, W_hq) + b_q\n        outputs.append(Y)\n    return outputs, (H, C)","execution_count":3},{"metadata":{"id":"9CA727FD0E904AA39A1C52F018BD37BF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 深度循环神经网络  \n\n![Image Name](https://cdn.kesci.com/upload/image/q5jk3z1hvz.png?imageView2/0/w/320/h/320)\n\n$$\n\\boldsymbol{H}_t^{(1)} = \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(1)} + \\boldsymbol{H}_{t-1}^{(1)} \\boldsymbol{W}_{hh}^{(1)} + \\boldsymbol{b}_h^{(1)})\\\\\n\\boldsymbol{H}_t^{(\\ell)} = \\phi(\\boldsymbol{H}_t^{(\\ell-1)} \\boldsymbol{W}_{xh}^{(\\ell)} + \\boldsymbol{H}_{t-1}^{(\\ell)} \\boldsymbol{W}_{hh}^{(\\ell)} + \\boldsymbol{b}_h^{(\\ell)})\\\\\n\\boldsymbol{O}_t = \\boldsymbol{H}_t^{(L)} \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n$$"},{"metadata":{"id":"D49649152A84435CB8057457658FA456","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 双向循环神经网络 \n\n![Image Name](https://cdn.kesci.com/upload/image/q5j8hmgyrz.png?imageView2/0/w/320/h/320)\n\n$$ \n\\begin{aligned} \\overrightarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(f)} + \\overrightarrow{\\boldsymbol{H}}_{t-1} \\boldsymbol{W}_{hh}^{(f)} + \\boldsymbol{b}_h^{(f)})\\\\\n\\overleftarrow{\\boldsymbol{H}}_t &= \\phi(\\boldsymbol{X}_t \\boldsymbol{W}_{xh}^{(b)} + \\overleftarrow{\\boldsymbol{H}}_{t+1} \\boldsymbol{W}_{hh}^{(b)} + \\boldsymbol{b}_h^{(b)}) \\end{aligned} $$\n$$\n\\boldsymbol{H}_t=(\\overrightarrow{\\boldsymbol{H}}_{t}, \\overleftarrow{\\boldsymbol{H}}_t)\n$$\n$$\n\\boldsymbol{O}_t = \\boldsymbol{H}_t \\boldsymbol{W}_{hq} + \\boldsymbol{b}_q\n$$"},{"metadata":{"id":"8CC05C1EB07A41CC9C0A884549B51AC0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 机器翻译\n机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。\n主要特征：输出是单词序列而不是单个单词。 \n困难：输出序列的长度可能与源序列的长度不同。\n### 数据处理\n1.数据清洗：处理乱码与空格。\n2.分词：将字符串转换成单词组成的列表\n3.建立词典，将单词组成的列表编程单词id组成的列表，这里会得到如下几样东西\n\t\t> 去重后词典，及其中单词对应的索引列表\n\t\t> 还可以得到给定索引找到其对应的单词的列表，以及给定单词得到对应索引的字典。\n\t\t> 原始语料所有词对应的词典索引的列表\n4.padding:一个batch中所有句子输入长度保持一致\n\t```\n\tdef pad(line, max_len, padding_token):\n\t\t\tif len(line) > max_len:\n\t\t\t\t\treturn line[:max_len]\n\t\t\treturn line + [padding_token] * (max_len - len(line))\n\t```\n5.加载数据：数据生成器\n\n### Encoder-Decoder\nencoder：输入到隐藏状态  \ndecoder：隐藏状态到输出\n对话系统、生成式任务、翻译\n\n####  Sequence to Sequence模型\nseq2seq模型基于编码器-解码器体系结构，通过序列输入生成序列输出。 \n编码器和解码器都使用递归神经网络（RNN）处理可变长度的序列输入。 \n编码器的隐藏状态直接用于初始化解码器的隐藏状态，以将信息从编码器传递到解码器。\n\n训练  \n![Image Name](https://cdn.kesci.com/upload/image/q5jc7a53pt.png?imageView2/0/w/640/h/640)\n预测\n![Image Name](https://cdn.kesci.com/upload/image/q5jcecxcba.png?imageView2/0/w/640/h/640)\n具体结构：\n![Image Name](https://cdn.kesci.com/upload/image/q5jccjhkii.png?imageView2/0/w/500/h/500)\n\nEncoder\n```\nclass Seq2SeqEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqEncoder, self).__init__(**kwargs)\n        self.num_hiddens=num_hiddens\n        self.num_layers=num_layers\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n   \n    def begin_state(self, batch_size, device):\n        return [torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device),\n                torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),  device=device)]\n    def forward(self, X, *args):\n        X = self.embedding(X) # X shape: (batch_size, seq_len, embed_size)\n        X = X.transpose(0, 1)  # RNN needs first axes to be time\n        # state = self.begin_state(X.shape[1], device=X.device)\n        out, state = self.rnn(X)\n        # The shape of out is (seq_len, batch_size, num_hiddens).\n        # state contains the hidden state and the memory cell\n        # of the last time step, the shape is (num_layers, batch_size, num_hiddens)\n        return out, state\n```\nDecoder\n```\nclass Seq2SeqDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqDecoder, self).__init__(**kwargs)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size,num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Linear(num_hiddens,vocab_size)\n\n    def init_state(self, enc_outputs, *args):\n        return enc_outputs[1]\n\n    def forward(self, X, state):\n        X = self.embedding(X).transpose(0, 1)\n        out, state = self.rnn(X, state)\n        # Make the batch to be the first dimension to simplify loss computation.\n        out = self.dense(out).transpose(0, 1)\n        return out, state\n```\n\n### 损失函数\n解码器的输出是一个和词典维度相同的向量，其每个值对应与向量索引位置对应词的分数，一般是选择分数最大的那个词作为最终的输出。\n在计算损失函数之前，要把padding去掉，因为padding的部分不参与计算\n#### 序列屏蔽\n序列有效长度保留，无效长度填充为特定value\n```\ndef SequenceMask(X, X_len,value=0):\n    maxlen = X.size(1)\n    mask = torch.arange(maxlen)[None, :].to(X_len.device) < X_len[:, None]   \n    X[~mask]=value\n    return X\n```\n#### 损失函数MaskedSoftmaxCELoss\n在交叉熵的基础上加上SequenceMask\n```\nclass MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n    # pred shape: (batch_size, seq_len, vocab_size)\n    # label shape: (batch_size, seq_len)\n    # valid_length shape: (batch_size, )\n    def forward(self, pred, label, valid_length):\n        # the sample weights shape should be (batch_size, seq_len)\n        weights = torch.ones_like(label)\n        weights = SequenceMask(weights, valid_length).float()\n        self.reduction='none'\n        output=super(MaskedSoftmaxCELoss, self).forward(pred.transpose(1,2), label)\n        return (output*weights).mean(dim=1)\n```\n\n###  Beam Search搜索算法\ngreedy search:只考虑当前时刻的局部最优解，没有考虑前后语义是否连贯（非全局最优解)\n维特比算法:选择整体分数最高的句子（搜索空间太大）\n集束搜索：结合了greedy search和维特比算法\n![Image Name](https://cdn.kesci.com/upload/image/q5jcia86z1.png?imageView2/0/w/640/h/640)\n"},{"metadata":{"id":"46957AA95325436E97DD9446BB181F5D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 注意力机制\n### seq2seq的不足\n解码器在各个时间步依赖相同的背景变量（context vector）来获取输⼊序列信息。当编码器为循环神经⽹络时，背景变量来⾃它最终时间步的隐藏状态。将源序列输入信息以循环单位状态编码，然后将其传递给解码器以生成目标序列。然而这种结构存在着问题，尤其是RNN机制实际中存在长程梯度消失的问题，对于较长的句子，我们很难寄希望于将输入的序列转化为定长的向量而保存所有的有效信息，**所以随着所需翻译句子的长度的增加，这种结构的效果会显著下降**。\n与此同时，解码的目标词语可能只与原输入的部分词语有关，而并不是与所有的输入有关。例如，当把“Hello world”翻译成“Bonjour le monde”时，“Hello”映射成“Bonjour”，“world”映射成“monde”。在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。\n\n### 注意力机制框架\n\nAttention 是一种通用的带权池化方法，输入由两部分构成：询问（query）和键值对（key-value pairs）。$𝐤_𝑖∈ℝ^{𝑑_𝑘}, 𝐯_𝑖∈ℝ^{𝑑_𝑣}$. Query  $𝐪∈ℝ^{𝑑_𝑞}$ , attention layer得到输出与value的维度一致 $𝐨∈ℝ^{𝑑_𝑣}$. 对于一个query来说，attention layer 会与每一个key计算注意力分数并进行权重的归一化，输出的向量$o$则是value的加权求和，而每个key计算的权重与value一一对应。\n\n为了计算输出，我们首先假设有一个函数$\\alpha$ 用于计算query和key的相似性，然后可以计算所有的 attention scores $a_1, \\ldots, a_n$ by\n$$\na_i = \\alpha(\\mathbf q, \\mathbf k_i).\n$$\n我们使用 softmax函数 获得注意力权重：\n$$\nb_1, \\ldots, b_n = \\textrm{softmax}(a_1, \\ldots, a_n).\n$$\n最终的输出就是value的加权求和：\n$$\n\\mathbf o = \\sum_{i=1}^n b_i \\mathbf v_i.\n$$\n![Image Name](https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960)\n\n不同的attetion layer的区别在于score函数的选择，在本节的其余部分，我们将讨论两个常用的注意层 Dot-product Attention 和 Multilayer Perceptron Attention\n\n\n### 点积注意力\nThe dot product 假设query和keys有相同的维度, 即 $\\forall i, 𝐪,𝐤_𝑖 ∈ ℝ_𝑑 $. 通过计算query和key转置的乘积来计算attention score,通常还会除去 $\\sqrt{d}$ 减少计算出来的score对维度𝑑的依赖性，如下\n$$\n𝛼(𝐪,𝐤)=⟨𝐪,𝐤⟩/ \\sqrt{d} \n$$\n假设 $ 𝐐∈ℝ^{𝑚×𝑑}$ 有 $m$ 个query，$𝐊∈ℝ^{𝑛×𝑑}$ 有 $n$ 个keys. 我们可以通过矩阵运算的方式计算所有 $mn$ 个score：\n$$\n𝛼(𝐐,𝐊)=𝐐𝐊^𝑇/\\sqrt{d}\n$$\n 现在让我们实现这个层，它支持一批查询和键值对。此外，它支持作为正则化随机删除一些注意力权重.\n ```\n class DotProductAttention(nn.Module): \n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # query: (batch_size, #queries, d)\n    # key: (batch_size, #kv_pairs, d)\n    # value: (batch_size, #kv_pairs, dim_v)\n    # valid_length: either (batch_size, ) or (batch_size, xx)\n    def forward(self, query, key, value, valid_length=None):\n        d = query.shape[-1]\n        # set transpose_b=True to swap the last two dimensions of key\n        \n        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        print(\"attention_weight\\n\",attention_weights)\n        return torch.bmm(attention_weights, value)\n```\n\n### 多层感知机注意力\n在多层感知器中，我们首先将 query and keys 投影到  $ℝ^ℎ$ .为了更具体，我们将可以学习的参数做如下映射 \n$𝐖_𝑘∈ℝ^{ℎ×𝑑_𝑘}$ ,  $𝐖_𝑞∈ℝ^{ℎ×𝑑_𝑞}$ , and  $𝐯∈ℝ^h$ . 将score函数定义\n$$\n𝛼(𝐤,𝐪)=𝐯^𝑇tanh(𝐖_𝑘𝐤+𝐖_𝑞𝐪)\n$$\n. \n然后将key 和 value 在特征的维度上合并（concatenate），然后送至 a single hidden layer perceptron 这层中 hidden layer 为  ℎ  and 输出的size为 1 .隐层激活函数为tanh，无偏置.\n```\nclass MLPAttention(nn.Module):  \n    def __init__(self, units,ipt_dim,dropout, **kwargs):\n        super(MLPAttention, self).__init__(**kwargs)\n        # Use flatten=True to keep query's and key's 3-D shapes.\n        self.W_k = nn.Linear(ipt_dim, units, bias=False)\n        self.W_q = nn.Linear(ipt_dim, units, bias=False)\n        self.v = nn.Linear(units, 1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, valid_length):\n        query, key = self.W_k(query), self.W_q(key)\n        #print(\"size\",query.size(),key.size())\n        # expand query to (batch_size, #querys, 1, units), and key to\n        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n        features = query.unsqueeze(2) + key.unsqueeze(1)\n        #print(\"features:\",features.size())  #--------------开启\n        scores = self.v(features).squeeze(-1) \n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        return torch.bmm(attention_weights, value)\n```\n\n### 引入注意力机制的Seq2seq模型\n下图展示encoding 和decoding的模型结构，在时间步为t的时候。此刻attention layer保存着encodering看到的所有信息——即encoding的每一步输出。在decoding阶段，解码器的$t$时刻的隐藏状态被当作query，encoder的每个时间步的hidden states作为key和value进行attention聚合. Attetion model的输出当作成上下文信息context vector，并与解码器输入$D_t$拼接起来一起送到解码器：\n\n![Image Name](https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800)\n\n$$\nFig1具有注意机制的seq-to-seq模型解码的第二步\n$$\n下图展示了seq2seq机制的所以层的关系，下面展示了encoder和decoder的layer结构\n\n![Image Name](https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800)\n$$\nFig2具有注意机制的seq-to-seq模型中层结构\n$$\n#### 解码器\n   由于带有注意机制的seq2seq的编码器不变，所以在此处我们只关注解码器。我们添加了一个MLP注意层(MLPAttention)，它的隐藏大小与解码器中的LSTM层相同。然后我们通过从编码器传递三个参数来初始化解码器的状态:\n   \n- the encoder outputs of all timesteps：encoder输出的各个状态，被用于attetion layer的memory部分，有相同的key和values\n\n- the hidden state of the encoder’s final timestep：编码器最后一个时间步的隐藏状态，被用于初始化decoder 的hidden state\n\n- the encoder valid length: 编码器的有效长度，借此，注意层不会考虑编码器输出中的填充标记（Paddings）\n\n   在解码的每个时间步，我们使用解码器的最后一个RNN层的输出作为注意层的query。然后，将注意力模型的输出与输入嵌入向量连接起来，输入到RNN层。虽然RNN层隐藏状态也包含来自解码器的历史信息，但是attention model的输出显式地选择了enc_valid_len以内的编码器输出，这样attention机制就会尽可能排除其他不相关的信息。\n```\nclass Seq2SeqAttentionDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Linear(num_hiddens,vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_len, *args):\n        outputs, hidden_state = enc_outputs\n#         print(\"first:\",outputs.size(),hidden_state[0].size(),hidden_state[1].size())\n        # Transpose outputs to (batch_size, seq_len, hidden_size)\n        return (outputs.permute(1,0,-1), hidden_state, enc_valid_len)\n        #outputs.swapaxes(0, 1)\n        \n    def forward(self, X, state):\n        enc_outputs, hidden_state, enc_valid_len = state\n        #(\"X.size\",X.size())\n        X = self.embedding(X).transpose(0,1)\n#         print(\"Xembeding.size2\",X.size())\n        outputs = []\n        for l, x in enumerate(X):\n#             print(f\"\\n{l}-th token\")\n#             print(\"x.first.size()\",x.size())\n            # query shape: (batch_size, 1, hidden_size)\n            # select hidden state of the last rnn layer as query\n            query = hidden_state[0][-1].unsqueeze(1) # np.expand_dims(hidden_state[0][-1], axis=1)\n            # context has same shape as query\n#             print(\"query enc_outputs, enc_outputs:\\n\",query.size(), enc_outputs.size(), enc_outputs.size())\n            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)\n            # Concatenate on the feature dimension\n#             print(\"context.size:\",context.size())\n            x = torch.cat((context, x.unsqueeze(1)), dim=-1)\n            # Reshape x to (1, batch_size, embed_size+hidden_size)\n#             print(\"rnn\",x.size(), len(hidden_state))\n            out, hidden_state = self.rnn(x.transpose(0,1), hidden_state)\n            outputs.append(out)\n        outputs = self.dense(torch.cat(outputs, dim=0))\n        return outputs.transpose(0, 1), [enc_outputs, hidden_state,\n                                        enc_valid_len]\n```\n我们得到了相同的解码器输出形状，但是状态结构改变了。"},{"metadata":{"id":"A969F1B11EB04891A9BC2AB322873E2C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"\n"},{"metadata":{"id":"92245AE2951B4894BA5BE0C4D6AA4E4F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null},{"metadata":{"id":"0B3BF201030040DB82796568250E7F37","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"code","source":"参考：\nhttps://zhuanlan.zhihu.com/p/32481747\nhttps://zhuanlan.zhihu.com/p/32085405","execution_count":null,"outputs":[]},{"metadata":{"id":"1AF7CEA004554AFCAD84250772D8DF50","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}